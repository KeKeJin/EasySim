# Simulator (indoor environment)

This is a simulation for AV indoor navigation. Currently it supports one scenes, and several functionalities, including virtual reality, depth rendering, and segmentation camera rendering. 
In the future(by the end of July 2019), it will have more scenes, and it will also simulate a lidar sensor, and it will be ROS compatible.

## Getting Started

1. Install unity (recommend 2018 2.4 and above). [Windows](https://unity3d.com/get-unity/download/archive) [Linux](https://beta.unity3d.com/download/fe703c5165de/public_download.html)
2. Install [Git LFS](https://git-lfs.github.com/) (this should be as simple as `git lfs install`).
3. Clone this repository from Github:

    ```      git clone https://github.com/kekekekekekekekeke/simulator.git
    ```
      


### Additional Notes

You need a headset and controllers to activate the VR functionality.


## Deployment
``` TODO: how to make the simulator ROS compatible ``` 

## Built With

* [Unity](https://unity.com/) - The web framework used
* [VIVE](https://www.vive.com/us/) - The headset and controllers


## Authors

* **Ke Jin** - *Initial work* - [kekekekekekekekeke](https://github.com/kekekekekekekekeke)


## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details

## Acknowledgments

* cruise control: [USelfDrivingSimulator](https://github.com/EvanWY/USelfDrivingSimulator)
* depth rendering: [unity forum](https://answers.unity.com/questions/877170/render-scene-depth-to-a-texture.html)
* segmentation rendering: [unity ml-imageSynthesis](https://bitbucket.org/Unity-Technologies/ml-imagesynthesis/src/master/)
* Inspiration: [lgsvl](https://github.com/lgsvl/simulator)
* etc
